# Crypto Data Stream: AnÃ¡lisis de Mercado en Tiempo Real con Apache Kafka y Python

## ğŸš€ Resumen del Proyecto
En este proyecto presento una soluciÃ³n robusta y escalable para el streaming y anÃ¡lisis de datos de mercado de criptomonedas en tiempo real, como parte de mi exploracion a las arquitecturas de Data Streaming <3.

Este proyecto lo realicÃ© en el verano de 2025.

Aprovechando Apache Kafka para una ingesta de datos fiable y Python para el procesamiento, se capturan actualizaciones de precios en vivo desde la API de Binance.
Esto establece una arquitectura fundamental para el monitoreo de mercado en tiempo real, trading algorÃ­tmico o aplicaciones analÃ­ticas avanzadas. Esta configuraciÃ³n ejemplifica principios clave de ingenierÃ­a de datos a la vez que sienta las bases para el anÃ¡lisis y visualizaciÃ³n de datos de alto volumen en tiempo real.

## âœ¨ CaracterÃ­sticas Clave y TecnologÃ­as
Ingesta de Datos en Tiempo Real: Captura datos de precios de criptomonedas en vivo desde la API de Binance.

Procesamiento AsÃ­ncrono: Utiliza asyncio para un manejo eficiente de flujos de datos concurrentes.

Broker de Mensajes: Emplea Apache Kafka como un broker de mensajes de alto rendimiento y tolerante a fallos para una entrega de datos fiable.

SerializaciÃ³n de Datos: Serializa datos eficientemente usando json para los mensajes de Kafka.

ContenerizaciÃ³n: Usa Docker Compose para orquestar los servicios de Kafka, Zookeeper y los scripts Python (productor/consumidor), asegurando una implementaciÃ³n sencilla y consistencia del entorno.

Python: Lenguaje principal para la adquisiciÃ³n de datos (productor) y el procesamiento (consumidor).

## ğŸ› ï¸ Stack TecnolÃ³gico
Lenguajes: Python ğŸ

Plataforma de Streaming: Apache Kafka

OrquestaciÃ³n: Docker Compose, Docker

IntegraciÃ³n de API: Binance WebSocket API

LibrerÃ­as: websocket-client, json, asyncio, confluent-kafka (consumidor)

## ğŸ“‚ Estructura del Proyecto
```
crypto_data_Stream/
â”œâ”€â”€ docker-compose.yml              # Define los servicios para Kafka, Zookeeper, Productor, Consumidor
â”œâ”€â”€ producer.py                     # Script Python para obtener datos de Binance API y enviarlos a Kafka
â”œâ”€â”€ consumer.py                     # Script Python para consumir datos de Kafka y procesarlos
â””â”€â”€ README.md                       # DocumentaciÃ³n del proyecto (este archivo)
```

##ğŸš€ CÃ³mo Ejecutar Localmente
Para poner en marcha este flujo de datos criptogrÃ¡ficos en tiempo real en tu mÃ¡quina, y hacer tus experimentos jaja, sigue estos sencillos pasos:

1-Clona el repositorio(yo uso la terminal Bash)

git clone https://github.com/alexmamani01/crypto_data_Stream.git
cd crypto_data_Stream
AsegÃºrate de que Docker estÃ© en ejecuciÃ³n: Verifica que Docker Desktop o tu demonio de Docker estÃ© activo.

2-Inicia los servicios:

docker-compose up --build
Este comando:

ConstruirÃ¡ las imÃ¡genes Docker de producer y consumer.

IniciarÃ¡ un contenedor de Zookeeper (requerido por Kafka).

IniciarÃ¡ un contenedor de broker de Kafka.

IniciarÃ¡ el contenedor producer.py, que comenzarÃ¡ a obtener datos de Binance y a enviarlos al tema binance_topic de Kafka.

IniciarÃ¡ el contenedor consumer.py, que leerÃ¡ los mensajes del tema binance_topic y los imprimirÃ¡ en la consola (verÃ¡s el flujo de datos en tiempo real).

3-Monitorea la salida:
VerÃ¡s los logs del producer indicando que se estÃ¡n enviando datos, y los logs del consumer mostrando los datos de criptomonedas en tiempo real que se reciben y procesan.

4-DetÃ©n los servicios (si tenÃ©s que detener los servicios por algun problema ): 

docker-compose down
## ğŸ”® Posible mejoras Futuras
Almacenamiento de Datos: Integrar con una base de datos (ej. PostgreSQL, MongoDB, Data Lake) para el almacenamiento persistente de los datos transmitidos.

Dashboard en Tiempo Real: Conectar el flujo de Kafka a una herramienta de visualizaciÃ³n (ej. Power BI, Tableau, Grafana) para un dashboard en vivo.

DetecciÃ³n de AnomalÃ­as: Implementar algoritmos de detecciÃ³n de anomalÃ­as en tiempo real sobre los datos consumidos.Muy pronto estarÃ© aplicando esto

Escalabilidad: Explorar la escalabilidad de clÃºsteres de Kafka y el despliegue en plataformas cloud (AWS MSK, Confluent Cloud).

## ğŸ‘‹ Â¡Conectemos!
Â¡No dudes en explorar el cÃ³digo, contribuir o contactarme si tienes alguna pregunta o idea de colaboraciÃ³n!
Alexandra
